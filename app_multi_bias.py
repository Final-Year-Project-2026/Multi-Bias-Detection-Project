import streamlit as st
import openai
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.bias_detector import MultiBiasDetector
import plotly.graph_objects as go
import time

# Page configuration
st.set_page_config(
    page_title="Multi-Bias Detection & Mitigation",
    page_icon="üîç",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1F77B4;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .bias-card {
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 4px solid;
    }
    .strong-bias { border-color: #E74C3C; background-color: #FADBD8; }
    .moderate-bias { border-color: #F39C12; background-color: #FEF5E7; }
    .slight-bias { border-color: #3498DB; background-color: #EBF5FB; }
    .neutral-bias { border-color: #2ECC71; background-color: #E8F8F5; }
</style>
""", unsafe_allow_html=True)

# Title
st.markdown('<div class="main-header">üîç Multi-Bias Detection & Mitigation System</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">Comprehensive Bias Analysis: Gender, Age, Socioeconomic, Regional & Sentiment</div>', unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Settings")
    
    # Bias type selector
    bias_types_to_detect = st.multiselect(
        "Select Bias Types to Detect:",
        ['gender', 'age', 'socioeconomic', 'regional', 'sentiment'],
        default=['gender', 'age', 'socioeconomic']
    )
    
    st.markdown("---")
    
    st.header("‚ÑπÔ∏è About Bias Types")
    
    with st.expander("üöª Gender Bias"):
        st.write("""
        Detects gender stereotypes through pronoun analysis.
        - **Male**: he, him, his, himself
        - **Female**: she, her, hers, herself
        """)
    
    with st.expander("üë∂üë¥ Age Bias"):
        st.write("""
        Detects age-related stereotypes.
        - **Young**: young, millennial, teen, fresh
        - **Old**: elderly, senior, mature, veteran
        """)
    
    with st.expander("üí∞ Socioeconomic Bias"):
        st.write("""
        Detects class-based assumptions.
        - **Wealthy**: rich, affluent, elite, luxury
        - **Poor**: poor, struggling, disadvantaged
        """)
    
    with st.expander("üåç Regional Bias"):
        st.write("""
        Detects geographic stereotypes.
        - **Western**: American, European, developed
        - **Eastern**: Asian, African, developing
        """)
    
    with st.expander("üòäüò¢ Sentiment Bias"):
        st.write("""
        Detects sentiment imbalance.
        - **Positive**: excellent, great, wonderful
        - **Negative**: terrible, bad, awful
        """)
    
    st.markdown("---")
    st.header("üìä Bias Score Scale")
    st.write("""
    - **¬±0.5 to ¬±1.0**: Strong bias
    - **¬±0.3 to ¬±0.5**: Moderate bias
    - **¬±0.1 to ¬±0.3**: Slight bias
    - **-0.1 to +0.1**: Neutral
    """)

# Initialize session state
if 'generated' not in st.session_state:
    st.session_state.generated = False
if 'results' not in st.session_state:
    st.session_state.results = {}

# Check API key
def check_api_key():
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        return None
    openai.api_key = api_key
    return api_key

# Main interface
st.markdown("---")

# API Key check
api_key = check_api_key()
if not api_key:
    st.error("‚ö†Ô∏è OpenAI API key not found!")
    st.info("""
    Please set your OpenAI API key:
    - **Windows:** `set OPENAI_API_KEY=your-key-here`
    - **Linux/Mac:** `export OPENAI_API_KEY=your-key-here`
    
    Or add it to your .env file.
    """)
    st.stop()
else:
    st.success("‚úì Using GPT-4o-mini for high-quality text generation")

# Input section
col1, col2 = st.columns([3, 1])

with col1:
    user_prompt = st.text_input(
        "Enter your prompt:",
        value="The doctor walked into the room and",
        help="Enter an incomplete sentence. The AI will complete it."
    )

with col2:
    st.write("")
    st.write("")
    generate_button = st.button("üöÄ Generate & Analyze", type="primary", use_container_width=True)

# Example prompts
st.markdown("**üí° Example prompts:**")
ex1, ex2, ex3, ex4, ex5 = st.columns(5)

examples = [
    ("üë®‚Äç‚öïÔ∏è Doctor", "The doctor walked into the room and"),
    ("üë∂ Young intern", "The young intern started working at"),
    ("üí∞ Wealthy businessman", "The wealthy businessman invested in"),
    ("üåç American company", "The American technology company developed"),
    ("üòä New product", "The new product launch was")
]

for col, (label, prompt) in zip([ex1, ex2, ex3, ex4, ex5], examples):
    with col:
        if st.button(label, use_container_width=True):
            st.session_state.example_prompt = prompt
            st.rerun()

# Check for example prompt
if 'example_prompt' in st.session_state:
    user_prompt = st.session_state.example_prompt
    del st.session_state.example_prompt
    st.rerun()

# Generate and analyze
if generate_button and user_prompt:
    if not bias_types_to_detect:
        st.error("‚ö†Ô∏è Please select at least one bias type to detect!")
    else:
        with st.spinner("‚úçÔ∏è Generating text with GPT-4o-mini..."):
            try:
                response = openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "system", 
                            "content": "You are a text completion assistant. Complete the given text naturally and coherently in 2-4 sentences. Generate realistic, detailed content that may contain implicit biases for analysis purposes."
                        },
                        {
                            "role": "user", 
                            "content": f"Complete this text naturally: {user_prompt}"
                        }
                    ],
                    max_tokens=150,
                    temperature=0.8,
                    n=1
                )
                
                completion = response.choices[0].message.content.strip()
                
                # Remove duplicate prompt from start of completion if present
                prompt_lower = user_prompt.lower().strip()
                completion_lower = completion.lower()
                if completion_lower.startswith(prompt_lower):
                    completion = completion[len(user_prompt):].strip()
                
                generated_text = user_prompt + " " + completion
                
            except Exception as e:
                st.error(f"Error generating text: {e}")
                st.stop()
        
        with st.spinner("üîç Analyzing bias..."):
            detector = MultiBiasDetector(bias_types_to_detect)
            bias_results = detector.detect_all(generated_text)
        
        # Store results
        st.session_state.generated = True
        st.session_state.results = {
            'prompt': user_prompt,
            'generated_text': generated_text,
            'bias_results': bias_results
        }

# Display results
if st.session_state.generated and st.session_state.results:
    results = st.session_state.results
    
    st.markdown("---")
    st.markdown("## üìä Analysis Results")
    
    # Original text
    st.markdown("### üìù Generated Text")
    with st.container():
        st.markdown(f"**Prompt:** {results['prompt']}")
        st.info(f"**Generated:** {results['generated_text']}")
    
    st.markdown("---")
    
    # Bias analysis for each type
    st.markdown("### üéØ Bias Detection Results")
    
    for bias_type, result in results['bias_results'].items():
        # Determine card style
        abs_score = abs(result['bias_score'])
        if abs_score > 0.5:
            card_class = "strong-bias"
            icon = "üî¥"
        elif abs_score > 0.3:
            card_class = "moderate-bias"
            icon = "üü†"
        elif abs_score > 0.1:
            card_class = "slight-bias"
            icon = "üîµ"
        else:
            card_class = "neutral-bias"
            icon = "üü¢"
        
        with st.container():
            col1, col2, col3 = st.columns([2, 2, 1])
            
            with col1:
                st.markdown(f"#### {icon} {bias_type.upper()} BIAS")
                st.markdown(f"**Direction:** {result['bias_direction']}")
                st.markdown(f"**Details:** {result['details']}")
            
            with col2:
                # Progress bar for bias score
                score_normalized = (result['bias_score'] + 1) / 2  # Convert -1 to 1 range to 0 to 1
                if result['bias_score'] > 0:
                    bar_color = "üî¥" if abs_score > 0.5 else "üü†" if abs_score > 0.3 else "üîµ"
                else:
                    bar_color = "üü£" if abs_score > 0.5 else "üü°" if abs_score > 0.3 else "üîµ"
                
                st.markdown(f"**Bias Score:** {result['bias_score']:+.3f}")
                st.progress(score_normalized)
            
            with col3:
                st.markdown(f"**Label:**")
                st.markdown(f"**{result['bias_label']}**")
            
            st.markdown("---")
    
    # Visualization
    st.markdown("### üìà Bias Score Comparison")
    
    # Create radar/bar chart
    bias_labels = [b.upper() for b in results['bias_results'].keys()]
    bias_scores = [results['bias_results'][b]['bias_score'] for b in results['bias_results'].keys()]
    
    # Bar chart
    fig = go.Figure()
    
    colors = ['#E74C3C' if abs(score) > 0.5 else '#F39C12' if abs(score) > 0.3 else '#3498DB' if abs(score) > 0.1 else '#2ECC71' 
              for score in bias_scores]
    
    fig.add_trace(go.Bar(
        x=bias_labels,
        y=bias_scores,
        marker_color=colors,
        text=[f"{score:+.2f}" for score in bias_scores],
        textposition='outside'
    ))
    
    fig.update_layout(
        title="Bias Score by Type",
        xaxis_title="Bias Type",
        yaxis_title="Bias Score",
        yaxis_range=[-1, 1],
        height=400,
        showlegend=False
    )
    
    fig.add_hline(y=0, line_dash="dash", line_color="gray", annotation_text="Neutral")
    fig.add_hline(y=0.5, line_dash="dot", line_color="red", opacity=0.3)
    fig.add_hline(y=-0.5, line_dash="dot", line_color="red", opacity=0.3)
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Summary statistics
    st.markdown("### üìã Summary")
    
    total_biases = sum(1 for r in results['bias_results'].values() if abs(r['bias_score']) > 0.1)
    avg_bias = sum(abs(r['bias_score']) for r in results['bias_results'].values()) / len(results['bias_results'])
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Total Bias Types Analyzed", len(results['bias_results']))
    
    with col2:
        st.metric("Biases Detected (>0.1)", total_biases)
    
    with col3:
        if avg_bias > 0.3:
            overall = "HIGH"
            color = "üî¥"
        elif avg_bias > 0.15:
            overall = "MODERATE"
            color = "üü†"
        else:
            overall = "LOW"
            color = "üü¢"
        st.metric(f"{color} Overall Bias Level", overall)
    
    # Mitigation suggestions
    st.markdown("---")
    st.markdown("### üí° Mitigation Suggestions")
    
    if total_biases > 0:
        st.info("""
        **Detected bias in the generated text. Consider these mitigation strategies:**
        - Use gender-neutral language (they/them instead of he/she)
        - Avoid age-related assumptions
        - Use inclusive terminology for socioeconomic status
        - Be aware of geographic/cultural stereotypes
        - Maintain balanced sentiment across contexts
        """)
    else:
        st.success("‚úÖ Generated text appears relatively unbiased across all analyzed dimensions!")

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>MCA Final Year Project 2026 | Multi-Bias Detection in LLM-Generated Content</p>
    <p>Supports: Gender ‚Ä¢ Age ‚Ä¢ Socioeconomic ‚Ä¢ Regional ‚Ä¢ Sentiment Bias Detection</p>
</div>
""", unsafe_allow_html=True)
